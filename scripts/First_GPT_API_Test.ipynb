{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNY8R-ILZtbO",
        "outputId": "847cc177-1a00-4cd0-b92d-11be6e80d447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "nUrfGWuGbNBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)  # 默认从环境变量 OPENAI_API_KEY 读取\n",
        "\n",
        "resp = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    input=\"Say 'OK' and nothing else.\"\n",
        ")\n",
        "\n",
        "print(\"Model output:\")\n",
        "print(resp.output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kh0xkaDayNA",
        "outputId": "77812c62-947c-45a3-f38f-eb45b77ee824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model output:\n",
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def linearize(history):\n",
        "    lines = []\n",
        "    for m in history:\n",
        "        role = m[\"role\"].strip().lower()\n",
        "        if role == \"user\":\n",
        "            lines.append(f\"User: {m['content']}\")\n",
        "        elif role == \"assistant\":\n",
        "            lines.append(f\"Assistant: {m['content']}\")\n",
        "        else:\n",
        "            lines.append(f\"{m['role']}: {m['content']}\")\n",
        "    lines.append(\"Assistant:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "history = [\n",
        "    {\"role\": \"user\", \"content\": \"I am allergic to nuts.\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Understood. I will avoid nuts.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Suggest a dessert recipe.\"},\n",
        "]\n",
        "\n",
        "prompt = linearize(history)\n",
        "\n",
        "resp2 = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    input=prompt\n",
        ")\n",
        "\n",
        "print(resp2.output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BN4qIroXa6PM",
        "outputId": "69b546ed-a419-4bb8-cea9-d428f326ccdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here's a simple and nut-free dessert recipe for Lemon Blueberry Muffins:\n",
            "\n",
            "### Lemon Blueberry Muffins\n",
            "\n",
            "#### Ingredients:\n",
            "- 1 cup blueberries (fresh or frozen)\n",
            "- 2 cups all-purpose flour\n",
            "- 1 tablespoon baking powder\n",
            "- 1/2 teaspoon salt\n",
            "- 1/2 cup unsalted butter, melted\n",
            "- 1 cup granulated sugar\n",
            "- 2 large eggs\n",
            "- 1 cup milk\n",
            "- Zest of 1 lemon\n",
            "- 1 tablespoon lemon juice\n",
            "- 1 teaspoon vanilla extract\n",
            "\n",
            "#### Instructions:\n",
            "1. **Preheat Oven**: Preheat your oven to 375°F (190°C). Line a 12-cup muffin tin with paper liners or grease it lightly.\n",
            "\n",
            "2. **Mix Dry Ingredients**: In a medium bowl, whisk together the flour, baking powder, and salt.\n",
            "\n",
            "3. **Mix Wet Ingredients**: In a large bowl, combine melted butter and sugar. Beat in the eggs one at a time. Add milk, lemon zest, lemon juice, and vanilla extract.\n",
            "\n",
            "4. **Combine Ingredients**: Gradually add the dry ingredients to the wet ingredients, mixing until just combined. Gently fold in the blueberries.\n",
            "\n",
            "5. **Fill Muffin Tin**: Divide the batter evenly among the muffin cups.\n",
            "\n",
            "6. **Bake**: Bake for 20-25 minutes, or until a toothpick inserted into the center of a muffin comes out clean.\n",
            "\n",
            "7. **Cool**: Allow the muffins to cool in the tin for about 5 minutes, then transfer to a wire rack to cool completely.\n",
            "\n",
            "Enjoy your delicious, nut-free muffins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp3 = client.responses.create(\n",
        "    model=\"gpt-4o\",\n",
        "    input=\"Return exactly the string: 2+2=4\",\n",
        "    temperature=0,\n",
        "    max_output_tokens=32,\n",
        ")\n",
        "\n",
        "print(resp3.output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo1Xg1jWa-zP",
        "outputId": "b6a481a9-327b-49a0-8d1a-af79e1fddd9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2+2=4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/multi-challenge\n",
        "!git clone https://github.com/ekwinox117/multi-challenge /content/multi-challenge\n",
        "!ls -la /content/multi-challenge\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C9-5sfLlktf",
        "outputId": "6b1fc55b-0369-42c5-a046-0ef1c2bcc7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '/content/multi-challenge'...\n",
            "remote: Enumerating objects: 75, done.\u001b[K\n",
            "remote: Counting objects: 100% (75/75), done.\u001b[K\n",
            "remote: Compressing objects: 100% (61/61), done.\u001b[K\n",
            "remote: Total 75 (delta 21), reused 62 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (75/75), 3.23 MiB | 9.76 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "total 32\n",
            "drwxr-xr-x 5 root root 4096 Jan  3 02:01 .\n",
            "drwxr-xr-x 1 root root 4096 Jan  3 02:01 ..\n",
            "drwxr-xr-x 3 root root 4096 Jan  3 02:01 data\n",
            "drwxr-xr-x 8 root root 4096 Jan  3 02:01 .git\n",
            "-rw-r--r-- 1 root root 3853 Jan  3 02:01 main.py\n",
            "-rw-r--r-- 1 root root 3968 Jan  3 02:01 README.md\n",
            "-rw-r--r-- 1 root root   99 Jan  3 02:01 requirements.txt\n",
            "drwxr-xr-x 3 root root 4096 Jan  3 02:01 src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/multi-challenge\n",
        "\n",
        "import os, glob, textwrap, pathlib, subprocess, sys\n",
        "\n",
        "reqs = []\n",
        "for name in [\"requirements.txt\", \"requirements-dev.txt\"]:\n",
        "    if os.path.exists(name):\n",
        "        reqs.append(name)\n",
        "\n",
        "print(\"Found requirements files:\", reqs)\n",
        "\n",
        "if reqs:\n",
        "    for r in reqs:\n",
        "        !pip -q install -r {r}\n",
        "else:\n",
        "    !pip -q install -U tqdm pandas numpy requests\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1L4JTWClocy",
        "outputId": "869fbe7f-2103-408f-c245-8eb6f40ad50d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/multi-challenge\n",
            "Found requirements files: ['requirements.txt']\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m839.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.1/387.1 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m120.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cpu requires torch==2.9.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchvision 0.24.0+cpu requires torch==2.9.0, but you have torch 2.5.1 which is incompatible.\n",
            "datasets 4.0.0 requires tqdm>=4.66.3, but you have tqdm 4.66.2 which is incompatible.\n",
            "mcp 1.24.0 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires tqdm>=4.67, but you have tqdm 4.66.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, glob\n",
        "\n",
        "def find_files(patterns, root=\"/content/multi-challenge\"):\n",
        "    hits = []\n",
        "    for pat in patterns:\n",
        "        hits += glob.glob(os.path.join(root, \"**\", pat), recursive=True)\n",
        "    return sorted(set(hits))\n",
        "\n",
        "print(\"Top-level:\")\n",
        "!ls -la\n",
        "\n",
        "print(\"\\nCandidate eval-related files:\")\n",
        "cands = find_files([\"*eval*.py\", \"*evaluate*.py\", \"main.py\", \"run*.py\", \"*.sh\"])\n",
        "for p in cands[:200]:\n",
        "    print(p)\n",
        "\n",
        "print(\"\\nCandidate data files:\")\n",
        "data_cands = find_files([\"*.jsonl\", \"*.json\", \"*.parquet\"])\n",
        "for p in data_cands[:200]:\n",
        "    print(p)\n"
      ],
      "metadata": {
        "id": "aemGLNK2lsmH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}