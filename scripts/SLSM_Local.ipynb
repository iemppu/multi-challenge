{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLSM Wrapper - Local Environment\n",
    "\n",
    "This notebook runs the SLSM (Semantic-Level State Machine) wrapper locally.\n",
    "\n",
    "**Prerequisites:**\n",
    "1. Activate conda environment: `conda activate multichallenge`\n",
    "2. Create `.env` file in project root with `OPENAI_API_KEY=sk-...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/wenhel/multi-challenge\n",
      "Python path includes: /home/wenhel/multi-challenge\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Environment Setup (Local)\n",
    "# =========================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Set project root\n",
    "PROJECT_ROOT = Path(\"/home/wenhel/multi-challenge\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Python path includes: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded environment from: /home/wenhel/multi-challenge/.env\n",
      "OPENAI_API_KEY is set (starts with: sk-proj-...)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Load API Keys from .env\n",
    "# =========================\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load from .env file in project root\n",
    "env_path = PROJECT_ROOT / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(f\"Loaded environment from: {env_path}\")\n",
    "else:\n",
    "    print(f\"WARNING: .env file not found at {env_path}\")\n",
    "    print(\"Please create .env with OPENAI_API_KEY=sk-...\")\n",
    "\n",
    "# Verify API key is set\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "if api_key:\n",
    "    print(f\"OPENAI_API_KEY is set (starts with: {api_key[:8]}...)\")\n",
    "else:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Install/Check Dependencies\n",
    "# =========================\n",
    "# !pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Benchmark Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 273 conversations\n",
      "Type: <class 'src.conversation.Conversation'>\n"
     ]
    }
   ],
   "source": [
    "from src.data_loader import DataLoader\n",
    "\n",
    "BENCHMARK = \"data/benchmark_questions.jsonl\"\n",
    "\n",
    "dl = DataLoader(input_file=BENCHMARK)\n",
    "dl.load_data()\n",
    "\n",
    "conversations = dl.get_conversations()\n",
    "\n",
    "print(f\"Loaded {len(conversations)} conversations\")\n",
    "print(f\"Type: {type(conversations[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in message: dict_keys(['role', 'content'])\n",
      "Number of turns: 3\n"
     ]
    }
   ],
   "source": [
    "# Inspect first conversation structure\n",
    "conv = conversations[0]\n",
    "messages = conv.conversation  # list[{\"role\", \"content\"}]\n",
    "\n",
    "print(f\"Keys in message: {messages[0].keys()}\")\n",
    "print(f\"Number of turns: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLSM Implementation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FULL CONVERSATION (indexed) ===\n",
      "\n",
      "--- turn 0 | user ---\n",
      "Hello!  I am an International relations expert working at the UN headquarters. My work requires me to consistently meet with diplomats from various countries. I hate using taxis or public transportation in New York. I prefer venues that are within a 5-minute walk from the UN headquarters.\n",
      "\n",
      "--- turn 1 | assistant ---\n",
      "Hello! It's great to hear that you're looking for places near the UN headquarters in New York. Here are a few suggestions that are within a 5-minute walk:\n",
      "\n",
      "1. Dag Hammarskjold Plaza: This public park is just across the street from the UN headquarters. It's a great place for a peaceful walk or a quick meeting.\n",
      "\n",
      "2. The Delegates Dining Room: Located within the UN headquarters itself, this is a convenient place for a meeting over lunch or dinner.\n",
      "\n",
      "3. The Roosevelt Hotel: This historic hotel is just ...[truncated]\n",
      "\n",
      "--- turn 2 | user ---\n",
      " I am meeting a German diplomat on Friday. I am looking for a suitable place to have lunch with him, preferably an up-class restaurant. I do not need to other arrangements, particularly security arrangements. Can you indicate for me some places where I can hold that meeting?\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SLSM Sanity Check\n",
    "# =========================\n",
    "\n",
    "from src.data_loader import DataLoader\n",
    "from src.models.openai import OpenAIModel\n",
    "from src.slsm_wrapper import (\n",
    "    SLSMConfig,\n",
    "    SLSMController,\n",
    "    SLSMWrapper,\n",
    ")\n",
    "\n",
    "# Load conversations\n",
    "BENCHMARK = \"data/benchmark_questions.jsonl\"\n",
    "dl = DataLoader(input_file=BENCHMARK)\n",
    "dl.load_data()\n",
    "conversations = dl.get_conversations()\n",
    "\n",
    "# Pick one conversation\n",
    "conv = conversations[0]\n",
    "messages = conv.conversation\n",
    "\n",
    "# Print full conversation (indexed)\n",
    "print(\"=== FULL CONVERSATION (indexed) ===\")\n",
    "for i, m in enumerate(messages):\n",
    "    role = m.get(\"role\")\n",
    "    content = (m.get(\"content\") or \"\")\n",
    "    # Truncate long turns for readability\n",
    "    preview = content if len(content) <= 500 else content[:500] + \" ...[truncated]\"\n",
    "    print(f\"\\n--- turn {i} | {role} ---\\n{preview}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SLSM components initialized.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Initialize SLSM Components\n",
    "# =========================\n",
    "\n",
    "# Controller (cheap, fixed temperature)\n",
    "controller_llm = OpenAIModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temp=0\n",
    ")\n",
    "\n",
    "cfg = SLSMConfig(\n",
    "    inject=\"always\",   # Force injection for testing\n",
    "    note_max_items=6,\n",
    ")\n",
    "\n",
    "controller = SLSMController(controller_llm, cfg)\n",
    "wrapper = SLSMWrapper(controller, cfg)\n",
    "\n",
    "# Underlying model (tested model)\n",
    "underlying_llm = OpenAIModel(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    temp=0\n",
    ")\n",
    "\n",
    "print(\"SLSM components initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BASELINE ===\n",
      "Certainly! Here are a few upscale restaurants near the UN headquarters that would be suitable for a lunch meeting with a German diplomat:\n",
      "\n",
      "1. **The Modern**: Located at the Museum of Modern Art, The Modern offers a refined dining experience with contemporary American cuisine. It's a bit further than a 5-minute walk, but it's a top choice for an upscale meal.\n",
      "\n",
      "2. **Aquavit**: This Michelin-starred restaurant offers Scandinavian cuisine and is known for its elegant setting and exceptional service.\n",
      "\n",
      "=== SLSM ===\n",
      "Certainly! Here are some up-class restaurants within a 5-minute walk from the UN headquarters where you can have lunch with the German diplomat:\n",
      "\n",
      "1. **The Modern**: Located at the Museum of Modern Art, this Michelin-starred restaurant offers a refined dining experience with contemporary American cuisine. It's a bit of a walk but still within a reasonable distance.\n",
      "\n",
      "2. **Aquavit**: This two-Michelin-starred restaurant offers a sophisticated dining experience with a focus on modern Nordic cuisine.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Compare Baseline vs SLSM\n",
    "# =========================\n",
    "\n",
    "# Baseline response\n",
    "baseline_resp = underlying_llm.generate(messages)\n",
    "\n",
    "# SLSM wrapped response\n",
    "slsm_resp = wrapper.generate_last_turn(\n",
    "    underlying_llm=underlying_llm,\n",
    "    original_conversation=messages,\n",
    ")\n",
    "\n",
    "print(\"\\n=== BASELINE ===\")\n",
    "print(baseline_resp[:500])\n",
    "\n",
    "print(\"\\n=== SLSM ===\")\n",
    "print(slsm_resp[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAW STATE FACTS (controller output, pre-gating) ===\n",
      "[{'id': 'F1', 'text': 'User is an International relations expert working at the UN headquarters.', 'support_turns': [0], 'evidence': 'I am an International relations expert working at the UN headquarters.'}]\n",
      "\n",
      "=== FINAL MSG ROLES (first 6) ===\n",
      "['system', 'user', 'assistant', 'user']\n",
      "\n",
      "=== INJECTED NOTE (first 1200 chars) ===\n",
      "system [SLSM MEMORY NOTE]\n",
      "Constraints:\n",
      "- [satisfied] User prefers venues that are within a 5-minute walk from the UN headquarters.\n",
      "- [satisfied] User hates using taxis or public transportation in New York.\n",
      "- [satisfied] User is looking for an up-class restaurant for lunch with a German diplomat.\n",
      "- [satisfied] User does not need other arrangements, particularly security arrangements.\n",
      "User facts/preferences:\n",
      "- User is an International relations expert working at the UN headquarters.\n",
      "\n",
      "Follow the constraints above.\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Inspect SLSM State\n",
    "# =========================\n",
    "\n",
    "# Inspect state and injected note\n",
    "state = wrapper.track_state(messages)\n",
    "print(\"\\n=== RAW STATE FACTS (controller output, pre-gating) ===\")\n",
    "print(state.facts)\n",
    "\n",
    "msgs = wrapper.build_final_messages(messages, state)\n",
    "print(\"\\n=== FINAL MSG ROLES (first 6) ===\")\n",
    "print([m[\"role\"] for m in msgs[:6]])\n",
    "\n",
    "print(\"\\n=== INJECTED NOTE (first 1200 chars) ===\")\n",
    "print(msgs[0][\"role\"], msgs[0][\"content\"][:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run SLSM on Full Benchmark (First 50 Samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 273 conversations\n",
      "Running SLSM-controlled GPT-4o: 100%|█████████| 50/50 [1:03:41<00:00, 76.43s/it]\n",
      "\n",
      "Done. Results saved to:\n",
      "  data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini-v2.jsonl\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Run SLSM on Benchmark\n",
    "# =========================\n",
    "\n",
    "!/home/wenhel/miniconda3/envs/multichallenge/bin/python run_slsm_multichallenge_gpt4o.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini-v2_responses_first50.jsonl lines=50\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Prepare Baseline First 50\n",
    "# =========================\n",
    "import json\n",
    "\n",
    "src = \"data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini-v2.jsonl\"\n",
    "dst = \"data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini-v2_responses_first50.jsonl\"\n",
    "\n",
    "with open(src, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = [next(f) for _ in range(50)]\n",
    "\n",
    "with open(dst, \"w\", encoding=\"utf-8\") as g:\n",
    "    for line in lines:\n",
    "        g.write(line)\n",
    "\n",
    "print(f\"Wrote: {dst} lines={len(lines)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Judge Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating responses: 100%|█████████████████████| 50/50 [01:51<00:00,  2.22s/it]\n",
      "\n",
      "=== SCORES ===\n",
      "{\n",
      "  \"overall_score\": 3.863828176565721,\n",
      "  \"axis_scores\": {\n",
      "    \"INFERENCE_MEMORY\": 1.7699115044247788,\n",
      "    \"RELIABLE_VERSION_EDITING\": 2.4390243902439024,\n",
      "    \"SELF_COHERENCE\": 4.0,\n",
      "    \"INSTRUCTION_RETENTION\": 7.246376811594203\n",
      "  }\n",
      "}\n",
      "\n",
      "Saved: outputs_first50/gpt4o_baseline_judge_results.json\n",
      "Saved: outputs_first50/gpt4o_baseline_judge_results.csv\n",
      "Done. Baseline judge evaluation saved to outputs_first50/\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Evaluate Baseline\n",
    "# =========================\n",
    "import os\n",
    "os.makedirs(\"outputs_first50\", exist_ok=True)\n",
    "\n",
    "!/home/wenhel/miniconda3/envs/multichallenge/bin/python -m run_judge_eval \\\n",
    "  --responses data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini-v2_responses_first50.jsonl \\\n",
    "  --out_json outputs_first50/gpt4o_baseline_judge_results.json \\\n",
    "  --out_csv outputs_first50/gpt4o_baseline_judge_results.csv \\\n",
    "  --workers 1 \\\n",
    "  --attempts 1\n",
    "\n",
    "print(\"Done. Baseline judge evaluation saved to outputs_first50/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Fix SLSM Response Format\n",
    "# =========================\n",
    "import json\n",
    "\n",
    "src = \"data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini.jsonl\"\n",
    "dst = \"data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini_mcformat.jsonl\"\n",
    "\n",
    "n = 0\n",
    "with open(src, \"r\", encoding=\"utf-8\") as f, open(dst, \"w\", encoding=\"utf-8\") as g:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "\n",
    "        # Normalize keys\n",
    "        qid = obj.get(\"QUESTION_ID\", obj.get(\"question_id\", obj.get(\"qid\")))\n",
    "        if qid is None:\n",
    "            raise KeyError(f\"Missing question id in line: {obj.keys()}\")\n",
    "\n",
    "        resp = obj.get(\"RESPONSE\", obj.get(\"response\", obj.get(\"answer\")))\n",
    "        if resp is None:\n",
    "            raise KeyError(f\"Missing response text in line: {obj.keys()}\")\n",
    "\n",
    "        model = obj.get(\"MODEL\", obj.get(\"model\", \"UNKNOWN_MODEL\"))\n",
    "\n",
    "        out = {\n",
    "            \"QUESTION_ID\": qid,\n",
    "            \"MODEL\": model,\n",
    "            \"RESPONSE\": resp,\n",
    "        }\n",
    "        g.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "        n += 1\n",
    "\n",
    "print(f\"Wrote {n} lines -> {dst}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Evaluate SLSM\n",
    "# =========================\n",
    "\n",
    "!python -m run_judge_eval \\\n",
    "  --responses data/final_model_responses/gpt-4o-2024-08-06_slsm-gpt-4o-mini_mcformat.jsonl \\\n",
    "  --out_json outputs_first50/gpt4o_slsm_judge_results.json \\\n",
    "  --out_csv outputs_first50/gpt4o_slsm_judge_results.csv \\\n",
    "  --workers 1 \\\n",
    "  --attempts 1\n",
    "\n",
    "print(\"Done. SLSM judge results saved to outputs_first50/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Results: Baseline vs SLSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Compare Judge Results\n",
    "# =========================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure paths\n",
    "BASE_CSV = Path(\"outputs_first50/gpt4o_baseline_judge_results.csv\")\n",
    "SLSM_CSV = Path(\"outputs_first50/gpt4o_slsm_judge_results.csv\")\n",
    "\n",
    "if not BASE_CSV.exists() or not SLSM_CSV.exists():\n",
    "    print(\"Missing CSV files. Run evaluation cells first.\")\n",
    "else:\n",
    "    base = pd.read_csv(BASE_CSV)\n",
    "    slsm = pd.read_csv(SLSM_CSV)\n",
    "\n",
    "    print(f\"Baseline CSV: {BASE_CSV} | rows: {len(base)} | cols: {len(base.columns)}\")\n",
    "    print(f\"SLSM CSV    : {SLSM_CSV} | rows: {len(slsm)} | cols: {len(slsm.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Bootstrap CI Analysis\n",
    "# =========================\n",
    "\n",
    "# Find ID column\n",
    "id_candidates = [\"QUESTION_ID\", \"question_id\", \"qid\", \"id\"]\n",
    "id_col = None\n",
    "for c in id_candidates:\n",
    "    if c in base.columns and c in slsm.columns:\n",
    "        id_col = c\n",
    "        break\n",
    "\n",
    "if id_col is None:\n",
    "    commons = set(base.columns) & set(slsm.columns)\n",
    "    for c in commons:\n",
    "        if \"id\" in c.lower():\n",
    "            id_col = c\n",
    "            break\n",
    "\n",
    "assert id_col is not None, f\"Cannot find common ID column. baseline cols={list(base.columns)}\"\n",
    "\n",
    "# Merge\n",
    "df = base.merge(slsm, on=id_col, how=\"inner\", suffixes=(\"_base\", \"_slsm\"))\n",
    "assert len(df) > 0, \"Merged dataframe is empty\"\n",
    "print(f\"ID column: {id_col} | merged rows: {len(df)}\")\n",
    "\n",
    "# Detect metric pairs\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_cols = [c for c in num_cols if c != id_col]\n",
    "\n",
    "pairs = []\n",
    "for c in num_cols:\n",
    "    if c.endswith(\"_base\"):\n",
    "        c2 = c[:-5] + \"_slsm\"\n",
    "        if c2 in df.columns:\n",
    "            pairs.append((c, c2))\n",
    "\n",
    "print(f\"Found {len(pairs)} metric pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Calculate Metrics\n",
    "# =========================\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "def bootstrap_ci_mean_diff(x, y, n_boot=5000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Returns (mean_diff, ci_low, ci_high) for (y-x) using bootstrap.\n",
    "    \"\"\"\n",
    "    diffs = (y - x).astype(float)\n",
    "    n = diffs.shape[0]\n",
    "    idx = rng.integers(0, n, size=(n_boot, n))\n",
    "    boot_means = diffs[idx].mean(axis=1)\n",
    "    lo = np.quantile(boot_means, alpha/2)\n",
    "    hi = np.quantile(boot_means, 1 - alpha/2)\n",
    "    return float(diffs.mean()), float(lo), float(hi)\n",
    "\n",
    "rows = []\n",
    "for b, s in pairs:\n",
    "    metric = b[:-5]  # remove \"_base\"\n",
    "    x = df[b].to_numpy()\n",
    "    y = df[s].to_numpy()\n",
    "\n",
    "    base_mean = float(np.mean(x))\n",
    "    slsm_mean = float(np.mean(y))\n",
    "    dmean, dlo, dhi = bootstrap_ci_mean_diff(x, y)\n",
    "\n",
    "    win = float(np.mean(y > x))\n",
    "    tie = float(np.mean(y == x))\n",
    "    lose = float(np.mean(y < x))\n",
    "\n",
    "    rows.append({\n",
    "        \"metric\": metric,\n",
    "        \"baseline_mean\": base_mean,\n",
    "        \"slsm_mean\": slsm_mean,\n",
    "        \"delta_mean\": dmean,\n",
    "        \"delta_ci_low\": dlo,\n",
    "        \"delta_ci_high\": dhi,\n",
    "        \"win_rate\": win,\n",
    "        \"tie_rate\": tie,\n",
    "        \"lose_rate\": lose,\n",
    "        \"n\": int(len(x)),\n",
    "    })\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values(\"metric\").reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== METRIC SUMMARY ===\")\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Generate LaTeX Table\n",
    "# =========================\n",
    "\n",
    "def fmt(x, nd=3):\n",
    "    return f\"{x:.{nd}f}\"\n",
    "\n",
    "latex = []\n",
    "latex += [r\"\\begin{table}[t]\",\n",
    "          r\"\\centering\",\n",
    "          r\"\\small\",\n",
    "          r\"\\begin{tabular}{lrrrr}\",\n",
    "          r\"\\toprule\",\n",
    "          r\"Metric & Baseline & SLSM & $\\Delta$ (boot 95\\% CI) & Win-rate \\\\\",\n",
    "          r\"\\midrule\"]\n",
    "\n",
    "for _, r in summary.iterrows():\n",
    "    metric = r[\"metric\"]\n",
    "    base_m = fmt(r[\"baseline_mean\"])\n",
    "    slsm_m = fmt(r[\"slsm_mean\"])\n",
    "    d = fmt(r[\"delta_mean\"])\n",
    "    lo = fmt(r[\"delta_ci_low\"])\n",
    "    hi = fmt(r[\"delta_ci_high\"])\n",
    "    win = f\"{100*r['win_rate']:.1f}\" + r\"\\%\"\n",
    "    latex.append(f\"{metric} & {base_m} & {slsm_m} & {d} [{lo}, {hi}] & {win} \\\\\\\\\")\n",
    "\n",
    "latex += [r\"\\bottomrule\",\n",
    "          r\"\\end{tabular}\",\n",
    "          r\"\\caption{Judge scores: baseline GPT-4o vs SLSM-controlled GPT-4o}\",\n",
    "          r\"\\label{tab:mc_judge}\",\n",
    "          r\"\\end{table}\"]\n",
    "\n",
    "latex_str = \"\\n\".join(latex)\n",
    "print(\"\\n=== LaTeX TABLE ===\\n\")\n",
    "print(latex_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Save Results\n",
    "# =========================\n",
    "\n",
    "out_dir = Path(\"outputs_first50\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_path = out_dir / \"baseline_vs_slsm_summary_first50.csv\"\n",
    "tex_path = out_dir / \"baseline_vs_slsm_table_first50.tex\"\n",
    "\n",
    "summary.to_csv(summary_path, index=False)\n",
    "tex_path.write_text(latex_str, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Saved summary CSV: {summary_path}\")\n",
    "print(f\"Saved LaTeX table: {tex_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (multichallenge)",
   "language": "python",
   "name": "multichallenge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
